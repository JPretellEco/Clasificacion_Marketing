# ğŸ“Š Estrategias Inteligentes de Marketing con Ciencia de Datos

## ğŸŒ Contexto Reconstruido: Del Caos al Conocimiento

En una era donde los clics narran historias mÃ¡s que las palabras, *ClickStream, una plataforma emergente en el mundo del e-commerce, se enfrenta a un desafÃ­o que muchas compaÃ±Ã­as digitales conocen bien: **Â¿cÃ³mo convertir un mar de datos en decisiones inteligentes?*

ClickStream acumulÃ³ miles de registros sobre sesiones, dispositivos, bÃºsquedas y comportamientos de navegaciÃ³n de sus usuarios. Sin embargo, toda esa informaciÃ³n dormÃ­a en silencio. El marketing seguÃ­a siendo genÃ©rico, sin alma, sin direcciÃ³n. Fue entonces cuando se planteÃ³ una nueva estrategia: *entender para personalizar*. NaciÃ³ asÃ­ la necesidad de crear un sistema de segmentaciÃ³n de usuarios que permitiese campaÃ±as mÃ¡s certeras, empÃ¡ticas y rentables.

Mi misiÃ³n es convertir el ruido digital en una partitura de patrones que permita a ClickStream anticipar la intensidad de respuesta de cada usuario ante una campaÃ±a de marketing, clasificÃ¡ndolos en tres niveles: *bajo (1), medio (2) y alto (3)*.


## ğŸ›  MetodologÃ­a Aplicada



### 1. ğŸ“š Carga y UnificaciÃ³n de Datos

Se integraron datos del usuario como de su movimiento (user_train.csv, user_test.csv) con datos de comportamiento (session_train.csv, session_test.csv) mediante joins por user_id.

### 2. ğŸ§ª IngenierÃ­a de CaracterÃ­sticas

A partir del anÃ¡lisis exploratorio, se construyeron nuevas variables:

* NÃºmero de sesiones y promedio de duraciÃ³n por usuario.
* Diversidad de dispositivos, paÃ­ses y navegadores utilizados.
* GeneraciÃ³n demogrÃ¡fica inferida desde la edad (Gen Z, Millennial, Gen X, Boomer).

### 3. âš– Manejo del Desbalance

Se utilizÃ³ *SMOTE* para balancear las clases en la variable marketing_target, crucial para mejorar el recall y la generalizaciÃ³n del modelo.

### 4. ğŸ¤– Modelado

Se compararon tres algoritmos:

* *Random Forest* con max_depth=10, logrando el mejor balance entre bias y varianza.
* *RegresiÃ³n LogÃ­stica Multiclase*, que mostrÃ³ robustez pero menor performance.
* *Ãrbol de DecisiÃ³n*, con precisiÃ³n aceptable pero mÃ¡s propenso al overfitting.

### 5. ğŸ“ˆ ValidaciÃ³n

Se aplicÃ³:

* *Train/test split (80/20)* con evaluaciÃ³n usando F1 Score ponderado.
* *Curva de aprendizaje* para verificar si el modelo se beneficiaba de mÃ¡s datos.
* *ValidaciÃ³n cruzada (k=10)* para comprobar estabilidad.

### 6. ğŸ§® PredicciÃ³n

Las predicciones sobre el dataset de prueba (X_test) fueron almacenadas como lo exige el desafÃ­o en:

json
{
  "target": {
    "297": 1,
    "11": 3,
    "67": 3,
    ...
  }
}


Guardadas dentro del directorio /predictions/predictions.json.

---

## ğŸ“Š Resultados Finales

| Modelo              | Accuracy | Precision | Recall | F1 Score |
| ------------------- | -------- | --------- | ------ | -------- |
| Random Forest       | 0.87     | 0.88      | 0.87   | 0.87     |
| Logistic Regression | 0.73     | 0.73      | 0.73   |  0.73    |
| Decision Tree       | 0.80     | 0.80      | 0.80   | 0.80     |

El *Random Forest* demostrÃ³ ser el mejor clasificador, destacando por su capacidad para capturar relaciones no lineales sin perder generalizaciÃ³n. El modelo logrÃ³ una *F1 ponderada de 0.87* en validaciÃ³n cruzada, superando el benchmark esperado.

---

## ğŸ§­ ConclusiÃ³n

De este modelo aprendÃ­:
1. En producciÃ³n, pueden darte ya la data entrenada, esto me ayudÃ³ a solo revisar los datos y generar nuevas variables para retroalimentar el nuevo modelo.
2. Hay que tener cuidado con el oversampling, cuando apliquÃ© oversampling con boostrap, me generalizaba demasiado, al punto de obtener F1: 1.00
3. El ordern es muy importe, y la automatizaciÃ³n tambiÃ©n. En este modelo no utilicÃ© pipelines, pero para otro modelo serÃ© mÃ¡s organizado y harÃ© pipelines con scikit-learn.

Lo tÃ©cnico se fundiÃ³ con lo humano: segmentar usuarios no es dividir, sino entender. 

---

